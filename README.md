# music2dance
A feature-based audio mosaic tool that aligns music spectrums with iconic dance video clips using K-Nearest Neighbors (KNN) for a coherent audiovisual performance.  
The user chooses a song and can control which video plays during its runtime. Dance clips are automatically sync'd with audio to simulate dancing to user's chosen song.  

[![Watch the video](https://img.youtube.com/vi/4_gVm7ntco?s1=BEXC5C8ynUZ5sBXA/maxresdefault.jpg)](https://www.youtube.com/watch?v=4_gVm7ntco?s1=BEXC5C8ynUZ5sBXA)  

What does it mean for something to be “coherent”? And how does the medium affect what sort of meaning we take out of a performance? This project allowed me to explore these themes and more as I was presented with the challenge of matching popular songs to iconic dance scenes from movies. It was tough to connect ChucK to Processing, but the hardest part overall was getting a good performance for the video. Just like an instrument, I had to practice when to switch videos, when to turn on the rainbow effect, which videos to switch between, and how often. Some of the videos responded better to certain parts of the song, so I had to ensure those were playing when that point was reached. Just like an instrument, there were sequences that flowed well together, and those that didn’t.

But perhaps the most intriguing part for me was that, even with the videos all jumbled around and flashing quickly, one could still recognize that they were dancing and that the clip was from a certain movie. However, when sound is rearranged like that, it is almost completely incoherent and sounds like a fever dream. Why is it the case that visuals can be rearranged and repeated without loss of meaning, but sound cannot? I presume it has something to do with the temporal nature of music, how it is strictly connected to the time in which it plays. In fact, this connection has been explored quite extensively in music theory, with scholars arguing that music’s aesthetic experience comes from play between expectation and realization, which are bound to a temporal, unfolding process. That interplay is on full display with this musical mosaic, and in fact enforced, since the visuals re-introduce earlier themes of the music and give the listener a reminder of the past.

Overall, I had a very good time both making and performing with these tools, and I hope to keep expanding upon this and try it out with more songs and videos in the future.  
Special thanks to Ge Wang and Andrew Zhu Aday for help with this project. Take CS470, AI & Music!
